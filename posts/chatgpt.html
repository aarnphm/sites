<!DOCTYPE html>
<!--
/*************************************************************************
* Bop got your nose !!!
*
* Hehe
*
* Anw if you see a component you like ping @aarnphm on Discord I can try
* to send it your way. Have a wonderful day!
**************************************************************************/
-->
<html lang="fr">
  <head>
    <title>
      On ChatGPT and its pedagogical consequences - Aaron's notes
    </title>
    <meta charset="utf-8"/>
    <link rel="preconnect" href="https://fonts.googleapis.com"/>
    <link rel="preconnect" href="https://fonts.gstatic.com"/>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=JetBrains Mono&amp;family=Newsreader:wght@400;700&amp;family=Newsreader:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/>
    <link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin="true"/>
    <link rel="preconnect" href="https://plausible.io" crossorigin="true"/>
    <link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta name="og:site_name" content="Aaron's notes"/>
    <meta property="og:locale" content="fr-FR"/>
    <meta property="og:title" content="On ChatGPT and its pedagogical consequences - Aaron's notes"/>
    <meta property="og:type" content="website"/>
    <meta name="twitter:site" content="@aarnphm_"/>
    <meta name="twitter:creator" content="@aarnphm_"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:title" content="On ChatGPT and its pedagogical consequences - Aaron's notes"/>
    <meta name="twitter:description" content="And its implication on how we assess learning. an overview."/>
    <meta property="og:description" content="And its implication on how we assess learning. an overview."/>
    <meta property="og:image:type" content="image/webp"/>
    <meta property="og:image:alt" content="And its implication on how we assess learning. an overview."/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>
    <meta property="og:width" content="1200"/>
    <meta property="og:height" content="630"/>
    <meta property="og:image" content="https://aarnphm.xyz/static/social-images/posts-chatgpt.webp"/>
    <meta property="og:url" content="https:/aarnphm.xyz/posts/chatgpt"/>
    <meta property="og:image" content="https://aarnphm.xyz/static/social-images/posts-chatgpt.webp"/>
    <meta name="twitter:image" content="https://aarnphm.xyz/static/social-images/posts-chatgpt.webp"/>
    <meta property="twitter:domain" content="https://aarnphm.xyz/"/>
    <meta property="twitter:url" content="https:/aarnphm.xyz/posts/chatgpt"/>
    <link rel="icon" href="../static/icon.webp"/>
    <link rel="canonical" href="https:/aarnphm.xyz/posts/chatgpt"/>
    <meta name="description" content="And its implication on how we assess learning. an overview."/>
    <meta name="generator" content="Quartz"/>
    <link href="../index.css" rel="stylesheet" type="text/css" spa-preserve/>
    <style>
      .collapsible-header {
      position: relative;
      margin-left: -9px;
      }
      .collapsible-header:first-child {
      margin-top: 1rem;
      }
      .collapsible-header:last-child {
      margin-bottom: 1rem;
      }
      .collapsible-header[data-level=&quot;2&quot;] {
      margin-left: 1rem;
      }
      .collapsible-header[data-level=&quot;3&quot;] {
      margin-left: 1.5rem;
      }
      .collapsible-header[data-level=&quot;4&quot;] {
      margin-left: 1.9rem;
      }
      .collapsible-header[data-level=&quot;5&quot;] {
      margin-left: 2.2rem;
      }
      .collapsible-header[data-level=&quot;6&quot;] {
      margin-left: 2.4rem;
      }
      .collapsible-header .collapsed-dots {
      display: none;
      }
      .collapsible-header.collapsed {
      margin-bottom: 1rem;
      }
      .collapsible-header.collapsed .collapsed-dots {
      display: block;
      }
      .collapsible-header .header-controls {
      display: grid;
      grid-template-columns: auto 1fr;
      align-items: center;
      }
      .collapsible-header .header-controls .toggle-button {
      grid-column: 1;
      }
      .collapsible-header .toggle-button {
      background-color: transparent;
      border: none;
      text-align: left;
      cursor: pointer;
      padding: 0;
      color: var(--dark);
      display: inline-flex;
      align-items: center;
      }
      .collapsible-header .toggle-button:hover .circle-icon {
      display: none;
      }
      .collapsible-header .toggle-button:hover[aria-expanded=true] .collapse-icon {
      display: block;
      }
      .collapsible-header .toggle-button:hover[aria-expanded=true] .expand-icon {
      display: none;
      }
      .collapsible-header .toggle-button:hover[aria-expanded=false] .expand-icon {
      display: block;
      }
      .collapsible-header .toggle-button:hover[aria-expanded=false] .collapse-icon {
      display: none;
      }
      .collapsible-header .toggle-icons {
      display: grid;
      grid-template-columns: 1fr;
      }
      .collapsible-header .toggle-icons .circle-icon,
      .collapsible-header .toggle-icons .expand-icon,
      .collapsible-header .toggle-icons .collapse-icon {
      opacity: 0.85;
      grid-column: 1;
      grid-row: 1;
      }
      .collapsible-header .toggle-icons .circle-icon {
      display: block;
      }
      .collapsible-header .toggle-icons .expand-icon,
      .collapsible-header .toggle-icons .collapse-icon {
      display: none;
      }
      .collapsible-header.collapsed :where(h1, h2, h3, h4, h5, h6):not(.popover *) {
      display: inline-flex;
      align-items: center;
      }
      .collapsible-header :where(h1, h2, h3, h4, h5, h6):not(.popover *) {
      margin: 0;
      }
      .collapsible-header .collapsible-header-content-outer {
      padding-left: 9px;
      padding-bottom: 9px;
      }
      .collapsible-header .collapsible-header-content-outer .collapsible-header-content {
      border-left: 1px solid var(--lightgray);
      overflow: hidden;
      padding-left: 9px;
      max-height: none;
      }
      .collapsible-header .collapsible-header-content-outer .collapsible-header-content.collapsed {
      max-height: 0 !important;
      padding-top: 0;
      padding-bottom: 0;
      }
    </style>
    <link href="https://cdn.jsdelivr.net/npm/node-tikzjax@latest/css/fonts.css" rel="stylesheet" type="text/css" spa-preserve/>
    <link href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/>
    <script src="../prescript.js" type="application/javascript" spa-preserve></script>
    <script type="application/javascript" spa-preserve>
      const fetchData = fetch("../static/contentIndex.json").then(data => data.json())
    </script>
    <script type="application/javascript" spa-preserve>
      var T=Object.create;var g=Object.defineProperty;var R=Object.getOwnPropertyDescriptor;var j=Object.getOwnPropertyNames;var k=Object.getPrototypeOf,O=Object.prototype.hasOwnProperty;var q=(e,u)=>()=>(u||e((u={exports:{}}).exports,u),u.exports);var P=(e,u,t,F)=>{if(u&&typeof u=="object"||typeof u=="function")for(let o of j(u))!O.call(e,o)&&o!==t&&g(e,o,{get:()=>u[o],enumerable:!(F=R(u,o))||F.enumerable});return e};var U=(e,u,t)=>(t=e!=null?T(k(e)):{},P(u||!e||!e.__esModule?g(t,"default",{value:e,enumerable:!0}):t,e));var m=q((X,p)=>{"use strict";p.exports=W;function A(e){return e instanceof Buffer?Buffer.from(e):new e.constructor(e.buffer.slice(),e.byteOffset,e.length)}function W(e){if(e=e||{},e.circles)return $(e);let u=new Map;if(u.set(Date,D=>new Date(D)),u.set(Map,(D,i)=>new Map(F(Array.from(D),i))),u.set(Set,(D,i)=>new Set(F(Array.from(D),i))),e.constructorHandlers)for(let D of e.constructorHandlers)u.set(D[0],D[1]);let t=null;return e.proto?l:o;function F(D,i){let n=Object.keys(D),r=new Array(n.length);for(let c=0;c<n.length;c++){let s=n[c],a=D[s];typeof a!="object"||a===null?r[s]=a:a.constructor!==Object&&(t=u.get(a.constructor))?r[s]=t(a,i):ArrayBuffer.isView(a)?r[s]=A(a):r[s]=i(a)}return r}function o(D){if(typeof D!="object"||D===null)return D;if(Array.isArray(D))return F(D,o);if(D.constructor!==Object&&(t=u.get(D.constructor)))return t(D,o);let i={};for(let n in D){if(Object.hasOwnProperty.call(D,n)===!1)continue;let r=D[n];typeof r!="object"||r===null?i[n]=r:r.constructor!==Object&&(t=u.get(r.constructor))?i[n]=t(r,o):ArrayBuffer.isView(r)?i[n]=A(r):i[n]=o(r)}return i}function l(D){if(typeof D!="object"||D===null)return D;if(Array.isArray(D))return F(D,l);if(D.constructor!==Object&&(t=u.get(D.constructor)))return t(D,l);let i={};for(let n in D){let r=D[n];typeof r!="object"||r===null?i[n]=r:r.constructor!==Object&&(t=u.get(r.constructor))?i[n]=t(r,l):ArrayBuffer.isView(r)?i[n]=A(r):i[n]=l(r)}return i}}function $(e){let u=[],t=[],F=new Map;if(F.set(Date,n=>new Date(n)),F.set(Map,(n,r)=>new Map(l(Array.from(n),r))),F.set(Set,(n,r)=>new Set(l(Array.from(n),r))),e.constructorHandlers)for(let n of e.constructorHandlers)F.set(n[0],n[1]);let o=null;return e.proto?i:D;function l(n,r){let c=Object.keys(n),s=new Array(c.length);for(let a=0;a<c.length;a++){let C=c[a],E=n[C];if(typeof E!="object"||E===null)s[C]=E;else if(E.constructor!==Object&&(o=F.get(E.constructor)))s[C]=o(E,r);else if(ArrayBuffer.isView(E))s[C]=A(E);else{let B=u.indexOf(E);B!==-1?s[C]=t[B]:s[C]=r(E)}}return s}function D(n){if(typeof n!="object"||n===null)return n;if(Array.isArray(n))return l(n,D);if(n.constructor!==Object&&(o=F.get(n.constructor)))return o(n,D);let r={};u.push(n),t.push(r);for(let c in n){if(Object.hasOwnProperty.call(n,c)===!1)continue;let s=n[c];if(typeof s!="object"||s===null)r[c]=s;else if(s.constructor!==Object&&(o=F.get(s.constructor)))r[c]=o(s,D);else if(ArrayBuffer.isView(s))r[c]=A(s);else{let a=u.indexOf(s);a!==-1?r[c]=t[a]:r[c]=D(s)}}return u.pop(),t.pop(),r}function i(n){if(typeof n!="object"||n===null)return n;if(Array.isArray(n))return l(n,i);if(n.constructor!==Object&&(o=F.get(n.constructor)))return o(n,i);let r={};u.push(n),t.push(r);for(let c in n){let s=n[c];if(typeof s!="object"||s===null)r[c]=s;else if(s.constructor!==Object&&(o=F.get(s.constructor)))r[c]=o(s,i);else if(ArrayBuffer.isView(s))r[c]=A(s);else{let a=u.indexOf(s);a!==-1?r[c]=t[a]:r[c]=i(s)}}return u.pop(),t.pop(),r}}});var J=Object.hasOwnProperty;var h=U(m(),1),eu=(0,h.default)();function y(e){return e.document.body.dataset.slug}function I(e,u=100){let t=e.getBoundingClientRect();return t.top>=-u&&t.bottom<=(window.innerHeight||document.documentElement.clientHeight)+u}function _(e,u){let t=e.getBoundingClientRect(),F=u.getBoundingClientRect(),o=e.parentElement?.getBoundingClientRect()||t;return F.top-o.top}function V(e,u){let t=e.getBoundingClientRect(),F=u.getBoundingClientRect();return{min:0,max:t.height-F.height}}function L(e,u,t){let F=_(u,e),o=V(t,u);F=Math.max(F,Math.min(o.min,o.max)),u.style.top=`${F}px`}function f(e,u){let t=e.querySelectorAll("a[data-footnote-ref]"),F=document.querySelector(".sidenotes");if(F)for(let o of t){let l=o.getAttribute("href")?.replace("#","sidebar-"),D=F.querySelector(`.sidenote-element[id="${l}"]`);D&&(u?(D.classList.remove("in-view"),o.classList.remove("active"),D.classList.add("collapsed")):I(o)?(D.classList.add("in-view"),o.classList.add("active"),D.classList.remove("collapsed"),L(o,D,F)):D.classList.remove("collapsed"))}}var S=(e,u)=>`${y(e)}-${u}`;function x(e,u){return localStorage.getItem(S(e,u))}function v(e,u,t){localStorage.setItem(S(e,u),t)}function b(e,u,t,F){e.setAttribute("aria-expanded",F?"false":"true"),u.style.maxHeight=F?"0px":"inherit",e.classList.toggle("collapsed",F),u.classList.toggle("collapsed",F),t.classList.toggle("collapsed",F),f(u,F)}function d(){let e=document.querySelector(".center"),u=document.querySelector(".sidenotes");if(!e||!u)return;let t=0,F=e.children;Array.from(F).forEach(l=>{let D=l.getBoundingClientRect();t+=D.height});let o=window.getComputedStyle(e);t+=parseFloat(o.paddingTop)+parseFloat(o.paddingBottom)+parseFloat(o.marginTop)+parseFloat(o.marginBottom),e.style.minHeight=`${t}px`,u.style.height=`${t}px`,u.offsetHeight,requestAnimationFrame(()=>{let l=u.querySelectorAll(".sidenote-element"),D=Array.from(l).filter(i=>i.classList.contains("in-view"));for(let i of D){let n=i.id.replace("sidebar-",""),r=e.querySelector(`a[href="#${n}"]`);r&&L(r,i,u)}})}function H(e,u){let t;return(...F)=>{clearTimeout(t),t=setTimeout(()=>e(...F),u)}}var z=H(d,150);function M(e){let u=e.target;if(!u)return;let t=u.closest(".toggle-button");if(!t||u.parentElement.classList.contains("callout"))return;let F=t.id.replace("collapsible-header-","").replace("-toggle",""),o=document.querySelector(`section.collapsible-header[id="${F}"]`);if(!o)return;e.stopPropagation();let l=document.querySelector(`.collapsible-header-content[data-references="${t.id}"]`);if(!l)return;let D=t.getAttribute("aria-expanded")==="true";t.setAttribute("aria-expanded",D?"false":"true"),l.style.maxHeight=D?"0px":"inherit",l.classList.toggle("collapsed",D),o.classList.toggle("collapsed",D),t.classList.toggle("collapsed",D),f(l,D),v(window,t.id,D?"false":"true"),requestAnimationFrame(()=>{d(),z()})}function w(){let e=document.querySelectorAll(".collapsible-header");for(let t of e){let F=t.querySelector("button.toggle-button");if(F){F.addEventListener("click",M),window.addCleanup(()=>F.removeEventListener("click",M));let o=document.querySelector(`.collapsible-header-content[data-references="${F.id}"]`);if(o){let l=x(window,F.id);l&&b(F,o,t,l==="false")}}}let u=document.querySelectorAll("svg.blockquote-link");for(let t of u){let l=function(){window.spaNavigate(new URL(o,window.location.toString()))},o=t.parentElement.dataset.href;t.addEventListener("click",l),window.addCleanup(()=>t.removeEventListener("click",l))}}document.addEventListener("nav",w);window.addEventListener("resize",w);
    </script>
  </head>
  <body data-slug="posts/chatgpt" data-language="fr" data-menu="false" data-poem="false">
    <main id="quartz-root" class="page">
      <section id="quartz-body">
        <aside class="left sidebar">
          <div class="toc desktop-only" id="toc" data-layout="minimal">
            <nav id="toc-vertical">
              <button class="depth-0 toc-item" data-depth="0" data-href="#introduction" data-for="introduction" style="--animation-order:1;" aria-label="Introduction.">
                <div class="fill"></div>
                <div class="indicator">
                  Introduction.
                </div>
              </button>
              <button class="depth-0 toc-item" data-depth="0" data-href="#gartners-hype-cycle" data-for="gartners-hype-cycle" style="--animation-order:2;" aria-label="Gartner’s hype cycle.">
                <div class="fill"></div>
                <div class="indicator">
                  Gartner’s hype cycle.
                </div>
              </button>
              <button class="depth-0 toc-item" data-depth="0" data-href="#implications-of-machine-assisted-interfaces-and-its-pedagogical-consequences-for-student-learning-and-assessment" data-for="implications-of-machine-assisted-interfaces-and-its-pedagogical-consequences-for-student-learning-and-assessment" style="--animation-order:3;" aria-label="Implications of machine-assisted interfaces and its pedagogical consequences for student learning and assessment.">
                <div class="fill"></div>
                <div class="indicator">
                  Implications of machine-assisted interfaces and its pedagogical consequences for student learning and assessment.
                </div>
              </button>
              <button class="depth-1 toc-item" data-depth="1" data-href="#reference-label" data-for="reference-label" style="--animation-order:4;" aria-label="References">
                <div class="fill"></div>
                <div class="indicator">
                  References
                </div>
              </button>
              <button class="depth-1 toc-item" data-depth="1" data-href="#backlinks-label" data-for="backlinks-label" style="--animation-order:5;" aria-label="Liens retour">
                <div class="fill"></div>
                <div class="indicator">
                  Liens retour
                </div>
              </button>
            </nav>
          </div>
          <div class="toolbar">
            <div class="toolbar-content">
              <button class="toolbar-item pen-button" aria-label="Toggle toolbar" aria-expanded="false">
                <svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="var(--darkgray)" viewBox="0 0 256 256" class="pen-icon">
                  <path d="M227.31,73.37,182.63,28.68a16,16,0,0,0-22.63,0L36.69,152A15.86,15.86,0,0,0,32,163.31V208a16,16,0,0,0,16,16H92.69A15.86,15.86,0,0,0,104,219.31L227.31,96a16,16,0,0,0,0-22.63ZM92.69,208H48V163.31l88-88L180.69,120ZM192,108.68,147.31,64l24-24L216,84.68Z"></path>
                </svg>
              </button>
              <button class="toolbar-item " id="reader-button" aria-label="Toggle reader mode" data-active="false">
                <span class="tooltip">
                  Reader mode
                </span>
                <svg class="reader-icon" xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M12 6.253v13m0-13C10.832 5.477 9.246 5 7.5 5S4.168 5.477 3 6.253v13C4.168 18.477 5.754 18 7.5 18s3.332.477 4.5 1.253m0-13C13.168 5.477 14.754 5 16.5 5c1.747 0 3.332.477 4.5 1.253v13C19.832 18.477 18.247 18 16.5 18c-1.746 0-3.332.477-4.5 1.253"></path>
                </svg>
              </button>
              <button class="toolbar-item " id="pdf-button" aria-label="Export to PDF">
                <span class="tooltip">
                  Export PDF
                </span>
                <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
                  <polyline points="14 2 14 8 20 8"></polyline>
                  <line x1="16" y1="13" x2="8" y2="13"></line>
                  <line x1="16" y1="17" x2="8" y2="17"></line>
                  <line x1="10" y1="9" x2="8" y2="9"></line>
                </svg>
              </button>
              <button class="toolbar-item " id="graph-button" aria-label="Toggle global graph">
                <span class="tooltip">
                  Global graph
                </span>
                <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve">
                  <path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path>
                </svg>
              </button>
              <button class="toolbar-item " id="skew-button" aria-label="add a tiny bit of skew, cuz why not">
                <span class="tooltip">
                  Skew page
                </span>
                <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M3 4l18 16"></path>
                  <path d="M21 4H3"></path>
                  <path d="M21 20H3"></path>
                </svg>
              </button>
            </div>
          </div>
        </aside>
        <section class="center">
          <div class="page-header">
            <header>
              <nav class="breadcrumb-container" aria-label="breadcrumbs">
                <div class="breadcrumb-element">
                  <a href="../">
                    ~
                  </a>
                  <p>
                    /
                  </p>
                </div>
                <div class="breadcrumb-element">
                  <a href="../posts/">
                    p
                  </a>
                  <p>
                    /
                  </p>
                </div>
                <div class="breadcrumb-element">
                  <a href>
                    On ChatGPT and its pedagogical consequences
                  </a>
                </div>
              </nav>
              <div class="keybind" lang="fr">
                <kbd id="shortcut-key" data-mapping="[&quot;cmd--'&quot;,&quot;ctrl--'&quot;]">
                  ⌘ '
                </kbd>
                <div id="shortcut-container">
                  <div id="shortcut-space">
                    <div id="title">
                      raccourcis clavier
                    </div>
                    <ul id="shortcut-list">
                      <li>
                        <div id="shortcuts" data-key="⌘--/" data-value="recherche"></div>
                      </li>
                      <li>
                        <div id="shortcuts" data-key="⌘--\" data-value="page d'accueil"></div>
                      </li>
                      <li>
                        <div id="shortcuts" data-key="⌘--j" data-value="curius"></div>
                      </li>
                      <li>
                        <div id="shortcuts" data-key="⌘--b" data-value="lecteur"></div>
                      </li>
                      <li>
                        <div id="shortcuts" data-key="⌘--g" data-value="graphique"></div>
                      </li>
                      <li>
                        <div id="shortcuts" data-key="⌘--u" data-value="fausser"></div>
                      </li>
                    </ul>
                  </div>
                </div>
              </div>
              <div class="search">
                <button class="search-button" id="search-button">
                  <svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7">
                    <title>
                      Search
                    </title>
                    <g class="search-path" fill="none">
                      <path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path>
                      <circle cx="8" cy="8" r="7"></circle>
                    </g>
                  </svg>
                </button>
                <div id="search-container">
                  <div id="search-space">
                    <input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Rechercher quelque chose" placeholder="Rechercher quelque chose"/>
                    <div id="search-layout" data-preview="true"></div>
                  </div>
                </div>
              </div>
            </header>
            <div class="popover-hint">
              <hgroup>
                <h1 class="article-title">
                  On ChatGPT and its pedagogical consequences
                </h1>
                <p class="description">
                  And its implication on how we assess learning. an overview.
                </p>
              </hgroup>
              <ul class="content-meta">
                <li>
                  <span class="page-creation" title="Date de création du contenu de la page">
                    <em>
                      01 oct. 2024
                    </em>
                  </span>
                </li>
                <li>
                  <a href="../posts/chatgpt.html.md" target="_blank" rel="noopener noreferrer" class="llm-source" style="color: inherit;font-weight: inherit;text-decoration: underline">
                    <span title="voir https://github.com/AnswerDotAI/llms-txt">
                      llms.txt
                    </span>
                  </a>
                </li>
              </ul>
              <ul class="tags">
                <li>
                  <a href="../tags/engineer4a03" class="internal tag-link">
                    engineer4a03
                  </a>
                </li>
                <li>
                  <a href="../tags/fruit" class="internal tag-link">
                    fruit
                  </a>
                </li>
              </ul>
              <div class="spacer"></div>
            </div>
          </div>
          <article class="popover-hint">
            <p dir="auto">
              <em>
                The following in an excerpt of a paper I wrote for my coursework.
              </em>
            </p>
            <blockquote class="callout question is-collapsible is-collapsed" data-callout="question" data-callout-fold>
              <div class="callout-title" dir="auto">
                <div class="callout-icon" dir="auto"></div>
                <div class="callout-title-inner" dir="auto">
                  <p dir="auto">
                    Question
                  </p>
                </div>
                <div class="fold-callout-icon" dir="auto"></div>
              </div>
              <div class="callout-content" dir="auto">
                <p dir="auto">
                  In the context of Gartner’s hype cycle, what has been the trajectory of generative conversational AI?
                </p>
                <p dir="auto">
                  Should a format including generative conversational AI be introduced to replace traditional essay assignments in educational settings,
                  and if so, what are some potential implications for student learning and assessment?
                  <cite class id="citation--dwivedi2023102642--1">
                    (
                    <a href="#bib-dwivedi2023102642" data-no-popover data-bib class="internal alias">
                      <span class="indicator-hook"></span>
                      Dwivedi et al., 2023
                    </a>
                    )
                  </cite>
                </p>
              </div>
            </blockquote>
            <h2 id="introduction" dir="auto">
              Introduction.
              <a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#introduction" class="internal">
                <span class="indicator-hook"></span>
                <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                </svg>
              </a>
            </h2>
            <p dir="auto">
              Historically, Alan Turing’s seminal work “Computing Machinery and Intelligence” laid the foundation for exploring the possibilities of a thinking machine
              <cite class id="citation--10.1093/mind/lix.236.433--2">
                (
                <a href="#bib-10.1093/mind/lix.236.433" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  TURING, 1950
                </a>
                )
              </cite>
              .
              Subsequently, the development of
              <a href="../thoughts/Machine-learning" class="internal alias" data-slug="thoughts/Machine-learning">
                <span class="indicator-hook"></span>
                AI
              </a>
              had taken a symbolic approach — world representations through systems that utilise high-level symbols and manipulate tokens to arrive at a result commonly referred to as Good Old-Fashioned AI (GOFAI)
              <cite class id="citation--10.7551/mitpress/4626.001.0001--3">
                (
                <a href="#bib-10.7551/mitpress/4626.001.0001" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Haugeland, 1997
                </a>
                )
              </cite>
              .
            </p>
            <p dir="auto">
              While GOFAI showed promise through decision-tree
              <a href="../thoughts/reason" class="internal alias" data-slug="thoughts/reason">
                <span class="indicator-hook"></span>
                reasoning
              </a>
              , its limitations became apparent in the 1980s when the field entered “AI Winter.” This was likely due to the cynicism within the AI researchers’ community and a reduction in funding, which halted most research and development
              <cite class id="citation--handler2008avoidanotheraiwinter--4">
                (
                <a href="#bib-handler2008avoidanotheraiwinter" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Hendler, 2008
                </a>
                )
              </cite>
              .
            </p>
            <p dir="auto">
              However, given the rise of Moore’s Law and the exponential amount of computing and
              <a href="../thoughts/data" class="internal alias" data-slug="thoughts/data">
                <span class="indicator-hook"></span>
                data
              </a>
              available, a new approach to
              <a href="../thoughts/AGI" class="internal alias" data-slug="thoughts/AGI">
                <span class="indicator-hook"></span>
                AI
              </a>
              arose, focusing on statistical methods and connectionist networks such as artificial neural networks.
              <cite class id="citation--10.7551/mitpress/4626.001.0001--5">
                (
                <a href="#bib-10.7551/mitpress/4626.001.0001" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Haugeland, 1997
                </a>
                )
              </cite>
              dubbed this approach as New Fangled AI (NFAI). Fast forward to the
              <span class="katex">
                <span class="katex-mathml">
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <semantics>
                      <mrow>
                        <mn>
                          2
                        </mn>
                        <msup>
                          <mn>
                            1
                          </mn>
                          <mtext>
                            st
                          </mtext>
                        </msup>
                      </mrow>
                      <annotation encoding="application/x-tex">
                        21^{\text{st}}
                      </annotation>
                    </semantics>
                  </math>
                </span>
                <span class="katex-html" aria-hidden="true">
                  <span class="base">
                    <span class="strut" style="height:0.7936em;"></span>
                    <span class="mord">
                      2
                    </span>
                    <span class="mord">
                      <span class="mord">
                        1
                      </span>
                      <span class="msupsub">
                        <span class="vlist-t">
                          <span class="vlist-r">
                            <span class="vlist" style="height:0.7936em;">
                              <span style="top:-3.063em;margin-right:0.05em;">
                                <span class="pstrut" style="height:2.7em;"></span>
                                <span class="sizing reset-size6 size3 mtight">
                                  <span class="mord mtight">
                                    <span class="mord text mtight">
                                      <span class="mord mtight">
                                        st
                                      </span>
                                    </span>
                                  </span>
                                </span>
                              </span>
                            </span>
                          </span>
                        </span>
                      </span>
                    </span>
                  </span>
                </span>
              </span>
              century, ML has entered the mainstream through the rise of generative AI (GenAI).
            </p>
            <p dir="auto">
              This paper posits that GenAI currently occupies the “peak of inflated expectations”, approaching the “trough of disillusionment” on Gartner’s hype cycle. It will also examine the implications of machine-assisted interfaces beyond conversational UI and their pedagogical consequences for student learning and assessment.
            </p>
            <h2 id="gartners-hype-cycle" dir="auto">
              Gartner’s hype cycle.
              <a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#gartners-hype-cycle" class="internal">
                <span class="indicator-hook"></span>
                <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                </svg>
              </a>
            </h2>
            <p dir="auto">
              For context, applications such as ChatGPT are built on top of
              <a href="../thoughts/Transformers" class="internal" data-slug="thoughts/Transformers">
                <span class="indicator-hook"></span>
                Transformers
              </a>
              architecture and pre-trained on a large corpus of
              <a href="../thoughts/Language#representation" class="internal alias" data-slug="thoughts/Language">
                <span class="indicator-hook"></span>
                text
              </a>
              <cite class id="citation--brown2020languagemodelsfewshotlearners--6">
                (
                <a href="#bib-brown2020languagemodelsfewshotlearners" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Brown et al., 2020
                </a>
                )
              </cite>
              . Given
              an input sequence of tokens length
              <span class="katex">
                <span class="katex-mathml">
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <semantics>
                      <mrow>
                        <mi>
                          n
                        </mi>
                      </mrow>
                      <annotation encoding="application/x-tex">
                        n
                      </annotation>
                    </semantics>
                  </math>
                </span>
                <span class="katex-html" aria-hidden="true">
                  <span class="base">
                    <span class="strut" style="height:0.4306em;"></span>
                    <span class="mord mathnormal">
                      n
                    </span>
                  </span>
                </span>
              </span>
              , these systems will predict the next tokens at index
              <span class="katex">
                <span class="katex-mathml">
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <semantics>
                      <mrow>
                        <mi>
                          n
                        </mi>
                        <mo>
                          +
                        </mo>
                        <mn>
                          1
                        </mn>
                      </mrow>
                      <annotation encoding="application/x-tex">
                        n+1
                      </annotation>
                    </semantics>
                  </math>
                </span>
                <span class="katex-html" aria-hidden="true">
                  <span class="base">
                    <span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span>
                    <span class="mord mathnormal">
                      n
                    </span>
                    <span class="mspace" style="margin-right:0.2222em;"></span>
                    <span class="mbin">
                      +
                    </span>
                    <span class="mspace" style="margin-right:0.2222em;"></span>
                  </span>
                  <span class="base">
                    <span class="strut" style="height:0.6444em;"></span>
                    <span class="mord">
                      1
                    </span>
                  </span>
                </span>
              </span>
              . Most implementations of transformers are autoregressive
              <cite class id="citation--croft2023llm--7">
                (
                <a href="#bib-croft2023llm" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Croft, 2023
                </a>
                )
              </cite>
              , meaning that the model will predict the future values (index
              <span class="katex">
                <span class="katex-mathml">
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <semantics>
                      <mrow>
                        <mi>
                          n
                        </mi>
                        <mo>
                          +
                        </mo>
                        <mn>
                          1
                        </mn>
                        <mo>
                          →
                        </mo>
                        <mi mathvariant="normal">
                          ∞
                        </mi>
                      </mrow>
                      <annotation encoding="application/x-tex">
                        n+1 \to \infty
                      </annotation>
                    </semantics>
                  </math>
                </span>
                <span class="katex-html" aria-hidden="true">
                  <span class="base">
                    <span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span>
                    <span class="mord mathnormal">
                      n
                    </span>
                    <span class="mspace" style="margin-right:0.2222em;"></span>
                    <span class="mbin">
                      +
                    </span>
                    <span class="mspace" style="margin-right:0.2222em;"></span>
                  </span>
                  <span class="base">
                    <span class="strut" style="height:0.6444em;"></span>
                    <span class="mord">
                      1
                    </span>
                    <span class="mspace" style="margin-right:0.2778em;"></span>
                    <span class="mrel">
                      →
                    </span>
                    <span class="mspace" style="margin-right:0.2778em;"></span>
                  </span>
                  <span class="base">
                    <span class="strut" style="height:0.4306em;"></span>
                    <span class="mord">
                      ∞
                    </span>
                  </span>
                </span>
              </span>
              ) based on past values (index
              <span class="katex">
                <span class="katex-mathml">
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <semantics>
                      <mrow>
                        <mn>
                          0
                        </mn>
                        <mo>
                          →
                        </mo>
                        <mi>
                          n
                        </mi>
                      </mrow>
                      <annotation encoding="application/x-tex">
                        0 \to n
                      </annotation>
                    </semantics>
                  </math>
                </span>
                <span class="katex-html" aria-hidden="true">
                  <span class="base">
                    <span class="strut" style="height:0.6444em;"></span>
                    <span class="mord">
                      0
                    </span>
                    <span class="mspace" style="margin-right:0.2778em;"></span>
                    <span class="mrel">
                      →
                    </span>
                    <span class="mspace" style="margin-right:0.2778em;"></span>
                  </span>
                  <span class="base">
                    <span class="strut" style="height:0.4306em;"></span>
                    <span class="mord mathnormal">
                      n
                    </span>
                  </span>
                </span>
              </span>
              ).
              However,
              <cite class id="citation--keles2022computationalcomplexityselfattention--8">
                (
                <a href="#bib-keles2022computationalcomplexityselfattention" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Keles et al., 2022, p. 4
                </a>
                )
              </cite>
              proved that the computation complexity of self-attention is quadratic; therefore, running these systems in production remains a scaling problem
              <cite class id="citation--kaplan2020scalinglawsneurallanguage--9">
                (
                <a href="#bib-kaplan2020scalinglawsneurallanguage" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Kaplan et al., 2020
                </a>
                )
              </cite>
              .
            </p>
            <p dir="auto">
              The current positioning of GenAI at the peak of inflated expectations aligns with the
              <cite class id="citation--gartner2024multimodal--10">
                (
                <a href="#bib-gartner2024multimodal" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Gartner, 2024
                </a>
                )
              </cite>
              prediction. Three key factors support this assessment: rapid advancement in research, widespread enterprise adoption, and increased public awareness.
              Ongoing research in GenAI, specifically language models, spans several topics, including mechanistic interpretability
              <cite class id="citation--nanda2023concrete--11">
                (
                <a href="#bib-nanda2023concrete" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Nanda, 2023
                </a>
                )
              </cite>
              , which explores the inner workings of auto-regressive models, information retrieval techniques aimed to improve correctness and reduce hallucinations among LLM systems
              <cite class id="citation--béchard2024reducinghallucinationstructuredoutputs--dhuliawala2023chainofverificationreduceshallucinationlarge--12">
                (
                <a href="#bib-béchard2024reducinghallucinationstructuredoutputs" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Béchard &amp; Ayala, 2024
                </a>
                ;
                <a href="#bib-dhuliawala2023chainofverificationreduceshallucinationlarge" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Dhuliawala et al., 2023
                </a>
                )
              </cite>
              ,
              as well as vested interests in multimodal applications of transformers
              <cite class id="citation--xu2023multimodallearningtransformerssurvey--13">
                (
                <a href="#bib-xu2023multimodallearningtransformerssurvey" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Xu et al., 2023
                </a>
                )
              </cite>
              . Leading research labs, from Anthropic on their interpretability and alignment work
              <cite class id="citation--elhage2022superposition--bricken2023monosemanticity--templeton2024scaling--14">
                (
                <a href="#bib-bricken2023monosemanticity" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Bricken et al., 2023
                </a>
                ;
                <a href="#bib-elhage2022superposition" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Elhage et al., 2022
                </a>
                ;
                <a href="#bib-templeton2024scaling" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Templeton et al., 2024
                </a>
                )
              </cite>
              , AI21’s Jamba with its innovative hybrid transformers architecture
              <cite class id="citation--jambateam2024jamba15hybridtransformermambamodels--15">
                (
                <a href="#bib-jambateam2024jamba15hybridtransformermambamodels" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Team et al., 2024
                </a>
                )
              </cite>
              to open-weights models from
              <a href="https://www.llama.com/" class="external alias">
                <span class="indicator-hook"></span>
                Meta
              </a>
              ,
              <a href="https://deepmind.google/technologies/gemini/pro/" class="external alias">
                <span class="indicator-hook"></span>
                Google
              </a>
              continue lead redefine the boundaries of what these systems are capable of.
            </p>
            <p dir="auto">
              Enterprise adoption is evident with Salesforce
              <cite class id="citation--nijkamp2023xgen7btechnicalreport--16">
                (
                <a href="#bib-nijkamp2023xgen7btechnicalreport" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Nijkamp et al., 2023
                </a>
                )
              </cite>
              , Oracle’s
              <a href="https://cohere.com/customer-stories/oracle" class="external alias">
                <span class="indicator-hook"></span>
                collaboration with Cohere
              </a>
              , and Microsoft’s Copilot for its 365 Product Suite. However, widespread implementation doesn’t necessarily equate to immediate, measurable productivity gains. Integrating these systems effectively into enterprise workflows to deliver tangible business value takes time and effort.
            </p>
            <p dir="auto">
              Despite the field’s excitement, the current hype and expectations often exceed its reliable capabilities, especially for complex use cases. Significant challenges persist, including
              hallucinations and lack of factual grounding
              <cite class id="citation--huang2023surveyhallucinationlargelanguage--17">
                (
                <a href="#bib-huang2023surveyhallucinationlargelanguage" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Huang et al., 2023, p. 3
                </a>
                )
              </cite>
              . We observe such behaviours in ChatGPT, where the given knowledge cutoff prevents the systems from providing up-to-date information, which will “hallucinate” and provide inaccurate answers.
              <cite class id="citation--dwivedi2023102642--18">
                (
                <a href="#bib-dwivedi2023102642" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Dwivedi et al., 2023, p. 4.4.9.1.2
                </a>
                )
              </cite>
            </p>
            <p dir="auto">
              As the field progresses towards the “trough of disillusionment” on Gartner’s hype cycle, a more realistic assessment of GenAI’s capabilities will likely emerge, paving the way for more effective applications.
            </p>
            <h2 id="implications-of-machine-assisted-interfaces-and-its-pedagogical-consequences-for-student-learning-and-assessment" dir="auto">
              Implications of machine-assisted interfaces and its pedagogical consequences for student learning and assessment.
              <a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#implications-of-machine-assisted-interfaces-and-its-pedagogical-consequences-for-student-learning-and-assessment" class="internal">
                <span class="indicator-hook"></span>
                <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                </svg>
              </a>
            </h2>
            <p dir="auto">
              The proliferation of conversational user interfaces (CUI) is based upon a simple heuristic of how
              <a href="../thoughts/Autoregressive-models" class="internal alias" data-slug="thoughts/Autoregressive-models">
                <span class="indicator-hook"></span>
                auto-regressive models
              </a>
              models surface their internal state through generating the next tokens.
            </p>
            <p dir="auto">
              CUIs often prove frustrating when dealing with tasks requiring larger information sets. Additionally, for tasks that require frequent information retrieval (research, academic writing), CUIs are suboptimal as they compel users to maintain information in their working memory unnecessarily.
              Pozdniakov proposed a framework that incorporate both application and interaction design, emphasizing manual alignment inputs from end users
              <cite class id="citation--pozdniakov2024largelanguagemodelsmeet--19">
                (
                <a href="#bib-pozdniakov2024largelanguagemodelsmeet" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Pozdniakov et al., 2024, p. 3
                </a>
                )
              </cite>
              .
              This approach, when applied replace traditional essay assignments, has two major implications for student learning and assessment: a shift in core competencies and collaborative assessment methods.
            </p>
            <p dir="auto">
              With machine-assisted interfaces, students will need to develop stronger critical thinking skills to evaluate AI-generated content and formulate precise instructions.
              The focus will shift towards the process of reaching desired outcomes and improving information retrieval skills. This shift aligns with the potential for machine-assisted proofs to solve novel problems, as discussed by
              <cite class id="citation--tao2024machineassisted--20">
                (
                <a href="#bib-tao2024machineassisted" data-no-popover data-bib class="internal alias">
                  <span class="indicator-hook"></span>
                  Tao, 2024
                </a>
                )
              </cite>
              .
            </p>
            <p dir="auto">
              These new interfaces will require instructors to adapt their evaluation methods. Assessment will need to consider students’ pace flexibility and their level of engagement with a given topic.
              This approach encourages a more holistic, cross-disciplinary understanding, better preparing students for continuous learning in our rapidly evolving technological landscape.
            </p>
            <section data-references class="bibliography">
              <h2 id="reference-label" dir="auto">
                References
                <a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#reference-label" class="internal">
                  <span class="indicator-hook"></span>
                  <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                  </svg>
                </a>
              </h2>
              <ul dir="auto">
                <li class="csl-entry" id="bib-béchard2024reducinghallucinationstructuredoutputs">
                  Béchard, P., &amp; Ayala, O. M. (2024).
                  <i>
                    Reducing hallucination in structured outputs via Retrieval-Augmented Generation
                  </i>
                  . arXiv preprint arXiv:2404.08189
                  <a href="https://arxiv.org/abs/2404.08189" target="_blank" rel="noopener noreferrer" class="csl-external-link internal alias" data-arxiv-id="2404.08189">
                    <span class="indicator-hook"></span>
                    arxiv
                  </a>
                </li>
                <li class="csl-entry" id="bib-bricken2023monosemanticity">
                  Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N., Anil, C., Denison, C., Askell, A., Lasenby, R., Wu, Y., Kravec, S., Schiefer, N., Maxwell, T., Joseph, N., Hatfield-Dodds, Z., Tamkin, A., Nguyen, K., … Olah, C. (2023). Towards Monosemanticity: Decomposing Language Models With Dictionary Learning.
                  <i>
                    Transformer Circuits Thread
                  </i>
                  .
                  <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html" target="_blank" rel="noopener noreferrer" class="csl-external-link external alias">
                    <span class="indicator-hook"></span>
                    [link]
                  </a>
                </li>
                <li class="csl-entry" id="bib-brown2020languagemodelsfewshotlearners">
                  Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020).
                  <i>
                    Language Models are Few-Shot Learners
                  </i>
                  . arXiv preprint arXiv:2005.14165
                  <a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener noreferrer" class="csl-external-link internal alias" data-arxiv-id="2005.14165">
                    <span class="indicator-hook"></span>
                    arxiv
                  </a>
                </li>
                <li class="csl-entry" id="bib-croft2023llm">
                  Croft, B. (2023).
                  <i>
                    LLM Visualization
                  </i>
                  .
                  <a href="https://bbycroft.net/llm" target="_blank" rel="noopener noreferrer" class="csl-external-link external">
                    <span class="indicator-hook"></span>
                    https://bbycroft.net/llm
                  </a>
                </li>
                <li class="csl-entry" id="bib-dhuliawala2023chainofverificationreduceshallucinationlarge">
                  Dhuliawala, S., Komeili, M., Xu, J., Raileanu, R., Li, X., Celikyilmaz, A., &amp; Weston, J. (2023).
                  <i>
                    Chain-of-Verification Reduces Hallucination in Large Language Models
                  </i>
                  . arXiv preprint arXiv:2309.11495
                  <a href="https://arxiv.org/abs/2309.11495" target="_blank" rel="noopener noreferrer" class="csl-external-link internal alias" data-arxiv-id="2309.11495">
                    <span class="indicator-hook"></span>
                    arxiv
                  </a>
                </li>
                <li class="csl-entry" id="bib-dwivedi2023102642">
                  Dwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, A. K., Baabdullah, A. M., Koohang, A., Raghavan, V., Ahuja, M., Albanna, H., Albashrawi, M. A., Al-Busaidi, A. S., Balakrishnan, J., Barlette, Y., Basu, S., Bose, I., Brooks, L., Buhalis, D., … Wright, R. (2023). Opinion Paper: “So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy.
                  <i>
                    International Journal of Information Management
                  </i>
                  ,
                  <i>
                    71
                  </i>
                  , 102642.
                  <a href="https://doi.org/https://doi.org/10.1016/j.ijinfomgt.2023.102642" target="_blank" rel="noopener noreferrer" class="csl-external-link external">
                    <span class="indicator-hook"></span>
                    https://doi.org/https://doi.org/10.1016/j.ijinfomgt.2023.102642
                  </a>
                </li>
                <li class="csl-entry" id="bib-elhage2022superposition">
                  Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S., Hatfield-Dodds, Z., Lasenby, R., Drain, D., Chen, C., Grosse, R., McCandlish, S., Kaplan, J., Amodei, D., Wattenberg, M., &amp; Olah, C. (2022). Toy Models of Superposition.
                  <i>
                    Transformer Circuits Thread
                  </i>
                  .
                  <a href="https://transformer-circuits.pub/2022/toy_model/index.html" target="_blank" rel="noopener noreferrer" class="csl-external-link external alias">
                    <span class="indicator-hook"></span>
                    [link]
                  </a>
                </li>
                <li class="csl-entry" id="bib-gartner2024multimodal">
                  Gartner. (2024).
                  <i>
                    Gartner Predicts 40 Percent of Generative AI Solutions Will Be Multimodal By 2027
                  </i>
                  .
                  <a href="https://www.gartner.com/en/newsroom/press-releases/2024-09-09-gartner-predicts-40-percent-of-generative-ai-solutions-will-be-multimodal-by-2027" target="_blank" rel="noopener noreferrer" class="csl-external-link external">
                    <span class="indicator-hook"></span>
                    https://www.gartner.com/en/newsroom/press-releases/2024-09-09-gartner-predicts-40-percent-of-generative-ai-solutions-will-be-multimodal-by-2027
                  </a>
                </li>
                <li class="csl-entry" id="bib-10.7551/mitpress/4626.001.0001">
                  Haugeland, J. (1997).
                  <i>
                    Mind Design II: Philosophy, Psychology, and Artificial Intelligence
                  </i>
                  . The MIT Press.
                  <a href="https://doi.org/10.7551/mitpress/4626.001.0001" target="_blank" rel="noopener noreferrer" class="csl-external-link external">
                    <span class="indicator-hook"></span>
                    https://doi.org/10.7551/mitpress/4626.001.0001
                  </a>
                </li>
                <li class="csl-entry" id="bib-handler2008avoidanotheraiwinter">
                  Hendler, J. (2008). Avoiding Another AI Winter.
                  <i>
                    IEEE Intelligent Systems
                  </i>
                  ,
                  <i>
                    23
                  </i>
                  (2), 2–4.
                  <a href="https://doi.org/10.1109/MIS.2008.20" target="_blank" rel="noopener noreferrer" class="csl-external-link external">
                    <span class="indicator-hook"></span>
                    https://doi.org/10.1109/MIS.2008.20
                  </a>
                </li>
                <li class="csl-entry" id="bib-huang2023surveyhallucinationlargelanguage">
                  Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., &amp; Liu, T. (2023).
                  <i>
                    A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions
                  </i>
                  . arXiv preprint arXiv:2311.05232
                  <a href="https://arxiv.org/abs/2311.05232" target="_blank" rel="noopener noreferrer" class="csl-external-link internal alias" data-arxiv-id="2311.05232">
                    <span class="indicator-hook"></span>
                    arxiv
                  </a>
                </li>
                <li class="csl-entry" id="bib-kaplan2020scalinglawsneurallanguage">
                  Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., &amp; Amodei, D. (2020).
                  <i>
                    Scaling Laws for Neural Language Models
                  </i>
                  . arXiv preprint arXiv:2001.08361
                  <a href="https://arxiv.org/abs/2001.08361" target="_blank" rel="noopener noreferrer" class="csl-external-link internal alias" data-arxiv-id="2001.08361">
                    <span class="indicator-hook"></span>
                    arxiv
                  </a>
                </li>
                <li class="csl-entry" id="bib-keles2022computationalcomplexityselfattention">
                  Keles, F. D., Wijewardena, P. M., &amp; Hegde, C. (2022).
                  <i>
                    On The Computational Complexity of Self-Attention
                  </i>
                  . arXiv preprint arXiv:2209.04881
                  <a href="https://arxiv.org/abs/2209.04881" target="_blank" rel="noopener noreferrer" class="csl-external-link internal alias" data-arxiv-id="2209.04881">
                    <span class="indicator-hook"></span>
                    arxiv
                  </a>
                </li>
                <li class="csl-entry" id="bib-nanda2023concrete">
                  Nanda, N. (2023).
                  <i>
                    Concrete Steps to Get Started in Transformer Mechanistic Interpretability
                  </i>
                  .
                  <a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started" target="_blank" rel="noopener noreferrer" class="csl-external-link external">
                    <span class="indicator-hook"></span>
                    https://www.neelnanda.io/mechanistic-interpretability/getting-started
                  </a>
                </li>
                <li class="csl-entry" id="bib-nijkamp2023xgen7btechnicalreport">
                  Nijkamp, E., Xie, T., Hayashi, H., Pang, B., Xia, C., Xing, C., Vig, J., Yavuz, S., Laban, P., Krause, B., Purushwalkam, S., Niu, T., Kryściński, W., Murakhovs’ka, L., Choubey, P. K., Fabbri, A., Liu, Y., Meng, R., Tu, L., … Xiong, C. (2023).
                  <i>
                    XGen-7B Technical Report
                  </i>
                  . arXiv preprint arXiv:2309.03450
                  <a href="https://arxiv.org/abs/2309.03450" target="_blank" rel="noopener noreferrer" class="csl-external-link internal alias" data-arxiv-id="2309.03450">
                    <span class="indicator-hook"></span>
                    arxiv
                  </a>
                </li>
                <li class="csl-entry" id="bib-pozdniakov2024largelanguagemodelsmeet">
                  Pozdniakov, S., Brazil, J., Abdi, S., Bakharia, A., Sadiq, S., Gasevic, D., Denny, P., &amp; Khosravi, H. (2024).
                  <i>
                    Large Language Models Meet User Interfaces: The Case of Provisioning Feedback
                  </i>
                  . arXiv preprint arXiv:2404.11072
                  <a href="https://arxiv.org/abs/2404.11072" target="_blank" rel="noopener noreferrer" class="csl-external-link internal alias" data-arxiv-id="2404.11072">
                    <span class="indicator-hook"></span>
                    arxiv
                  </a>
                </li>
                <li class="csl-entry" id="bib-tao2024machineassisted">
                  Tao, T. (2024).
                  <i>
                    Machine-Assisted Proofs
                  </i>
                  .
                  <a href="https://terrytao.wordpress.com/wp-content/uploads/2024/03/machine-assisted-proof-notices.pdf" target="_blank" rel="noopener noreferrer" class="csl-external-link external">
                    <span class="indicator-hook"></span>
                    https://terrytao.wordpress.com/wp-content/uploads/2024/03/machine-assisted-proof-notices.pdf
                  </a>
                </li>
                <li class="csl-entry" id="bib-jambateam2024jamba15hybridtransformermambamodels">
                  Team, J., Lenz, B., Arazi, A., Bergman, A., Manevich, A., Peleg, B., Aviram, B., Almagor, C., Fridman, C., Padnos, D., Gissin, D., Jannai, D., Muhlgay, D., Zimberg, D., Gerber, E. M., Dolev, E., Krakovsky, E., Safahi, E., Schwartz, E., … Shoham, Y. (2024).
                  <i>
                    Jamba-1.5: Hybrid Transformer-Mamba Models at Scale
                  </i>
                  . arXiv preprint arXiv:2408.12570
                  <a href="https://arxiv.org/abs/2408.12570" target="_blank" rel="noopener noreferrer" class="csl-external-link internal alias" data-arxiv-id="2408.12570">
                    <span class="indicator-hook"></span>
                    arxiv
                  </a>
                </li>
                <li class="csl-entry" id="bib-templeton2024scaling">
                  Templeton, A., Conerly, T., Marcus, J., Lindsey, J., Bricken, T., Chen, B., Pearce, A., Citro, C., Ameisen, E., Jones, A., Cunningham, H., Turner, N. L., McDougall, C., MacDiarmid, M., Freeman, C. D., Sumers, T. R., Rees, E., Batson, J., Jermyn, A., … Henighan, T. (2024). Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet.
                  <i>
                    Transformer Circuits Thread
                  </i>
                  .
                  <a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html" target="_blank" rel="noopener noreferrer" class="csl-external-link external alias">
                    <span class="indicator-hook"></span>
                    [link]
                  </a>
                </li>
                <li class="csl-entry" id="bib-10.1093/mind/lix.236.433">
                  TURING, A. M. (1950). I.—COMPUTING MACHINERY AND INTELLIGENCE.
                  <i>
                    Mind
                  </i>
                  ,
                  <i>
                    LIX
                  </i>
                  (236), 433–460.
                  <a href="https://doi.org/10.1093/mind/LIX.236.433" target="_blank" rel="noopener noreferrer" class="csl-external-link external">
                    <span class="indicator-hook"></span>
                    https://doi.org/10.1093/mind/LIX.236.433
                  </a>
                </li>
                <li class="csl-entry" id="bib-xu2023multimodallearningtransformerssurvey">
                  Xu, P., Zhu, X., &amp; Clifton, D. A. (2023).
                  <i>
                    Multimodal Learning with Transformers: A Survey
                  </i>
                  . arXiv preprint arXiv:2206.06488
                  <a href="https://arxiv.org/abs/2206.06488" target="_blank" rel="noopener noreferrer" class="csl-external-link internal alias" data-arxiv-id="2206.06488">
                    <span class="indicator-hook"></span>
                    arxiv
                  </a>
                </li>
              </ul>
            </section>
          </article>
          <div class="page-footer">
            <section data-backlinks="true" class="backlinks">
              <h2 id="backlinks-label">
                Liens retour
                <a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#backlinks-label" class="internal">
                  <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                  </svg>
                </a>
              </h2>
              <div class="overflow">
                <a href="../thoughts/university/twenty-four-twenty-five/engineer-4a03/literature-review" data-backlink="thoughts/university/twenty-four-twenty-five/engineer-4a03/literature-review">
                  <div class="small">
                    machine learning, as inception of time, a literature review
                  </div>
                  <div class="description">
                    How we understand machine learning system is how we can move towards a safe futures, yet the road ahead lies many troubles to overcome. A literature review into the inception of the field, as well as where do we go from here.
                  </div>
                </a>
              </div>
            </section>
            <footer class="minimal-footer">
              <ul class="icons">
                <li>
                  <a href="/" target="_self" aria-label="home">
                    <svg viewBox="64 64 896 896" focusable="false" data-icon="home" width="1em" height="1em" fill="currentColor" aria-hidden="true">
                      <path d="M946.5 505L560.1 118.8l-25.9-25.9a31.5 31.5 0 00-44.4 0L77.5 505a63.9 63.9 0 00-18.8 46c.4 35.2 29.7 63.3 64.9 63.3h42.5V940h691.8V614.3h43.4c17.1 0 33.2-6.7 45.3-18.8a63.6 63.6 0 0018.7-45.3c0-17-6.7-33.1-18.8-45.2zM568 868H456V664h112v204zm217.9-325.7V868H632V640c0-22.1-17.9-40-40-40H432c-22.1 0-40 17.9-40 40v228H238.1V542.3h-96l370-369.7 23.1 23.1L882 542.3h-96.1z"></path>
                    </svg>
                  </a>
                </li>
                <li>
                  <a href="https://github.com/aarnphm" target="_blank" aria-label="github">
                    <svg viewBox="64 64 896 896" focusable="false" data-icon="github" width="1em" height="1em" fill="var(--gray)" aria-label="true">
                      <path d="M511.6 76.3C264.3 76.2 64 276.4 64 523.5 64 718.9 189.3 885 363.8 946c23.5 5.9 19.9-10.8 19.9-22.2v-77.5c-135.7 15.9-141.2-73.9-150.3-88.9C215 726 171.5 718 184.5 703c30.9-15.9 62.4 4 98.9 57.9 26.4 39.1 77.9 32.5 104 26 5.7-23.5 17.9-44.5 34.7-60.8-140.6-25.2-199.2-111-199.2-213 0-49.5 16.3-95 48.3-131.7-20.4-60.5 1.9-112.3 4.9-120 58.1-5.2 118.5 41.6 123.2 45.3 33-8.9 70.7-13.6 112.9-13.6 42.4 0 80.2 4.9 113.5 13.9 11.3-8.6 67.3-48.8 121.3-43.9 2.9 7.7 24.7 58.3 5.5 118 32.4 36.8 48.9 82.7 48.9 132.3 0 102.2-59 188.1-200 212.9a127.5 127.5 0 0138.1 91v112.5c.8 9 0 17.9 15 17.9 177.1-59.7 304.6-227 304.6-424.1 0-247.2-200.4-447.3-447.5-447.3z"></path>
                    </svg>
                  </a>
                </li>
                <li>
                  <a href="https://twitter.com/aarnphm_" target="_blank" aria-label="twitter">
                    <svg viewBox="64 64 896 896" focusable="false" data-icon="twitter" width="1em" height="1em" fill="var(--gray)" aria-hidden="true">
                      <path d="M928 254.3c-30.6 13.2-63.9 22.7-98.2 26.4a170.1 170.1 0 0075-94 336.64 336.64 0 01-108.2 41.2A170.1 170.1 0 00672 174c-94.5 0-170.5 76.6-170.5 170.6 0 13.2 1.6 26.4 4.2 39.1-141.5-7.4-267.7-75-351.6-178.5a169.32 169.32 0 00-23.2 86.1c0 59.2 30.1 111.4 76 142.1a172 172 0 01-77.1-21.7v2.1c0 82.9 58.6 151.6 136.7 167.4a180.6 180.6 0 01-44.9 5.8c-11.1 0-21.6-1.1-32.2-2.6C211 652 273.9 701.1 348.8 702.7c-58.6 45.9-132 72.9-211.7 72.9-14.3 0-27.5-.5-41.2-2.1C171.5 822 261.2 850 357.8 850 671.4 850 843 590.2 843 364.7c0-7.4 0-14.8-.5-22.2 33.2-24.3 62.3-54.4 85.5-88.2z"></path>
                    </svg>
                  </a>
                </li>
                <li>
                  <a href="https://bsky.app/profile/aarnphm.xyz" target="_blank" aria-label="bsky">
                    <svg viewBox="0 0 512 512" focusable="false" data-icon="bsky" width="1em" height="1em" aria-hidden="true">
                      <path d="M111.8 62.2C170.2 105.9 233 194.7 256 242.4c23-47.6 85.8-136.4 144.2-180.2c42.1-31.6 110.3-56 110.3 21.8c0 15.5-8.9 130.5-14.1 149.2C478.2 298 412 314.6 353.1 304.5c102.9 17.5 129.1 75.5 72.5 133.5c-107.4 110.2-154.3-27.6-166.3-62.9l0 0c-1.7-4.9-2.6-7.8-3.3-7.8s-1.6 3-3.3 7.8l0 0c-12 35.3-59 173.1-166.3 62.9c-56.5-58-30.4-116 72.5-133.5C100 314.6 33.8 298 15.7 233.1C10.4 214.4 1.5 99.4 1.5 83.9c0-77.8 68.2-53.4 110.3-21.8z" fill="#1185fe"></path>
                    </svg>
                  </a>
                </li>
              </ul>
              <p class="info">
                Créé avec
                <a href="https://quartz.jzhao.xyz/" target="_blank" rel="noopener noreferrer" aria-label="Quartz links">
                  Quartz v4.4.0
                </a>
                © 2024
              </p>
            </footer>
          </div>
        </section>
        <section class="right sidebar">
          <div class="graph">
            <div id="global-graph-outer">
              <div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true}"></div>
            </div>
          </div>
          <div class="reader" id="reader-view">
            <div class="reader-backdrop"></div>
            <div class="reader-container">
              <div class="reader-header">
                <button class="reader-close" aria-label="Close reader">
                  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <line x1="18" y1="6" x2="6" y2="18"></line>
                    <line x1="6" y1="6" x2="18" y2="18"></line>
                  </svg>
                </button>
              </div>
              <div class="reader-content">
                <h1 class="reader-title">
                  On ChatGPT and its pedagogical consequences
                </h1>
                <p dir="auto">
                  <em>
                    The following in an excerpt of a paper I wrote for my coursework.
                  </em>
                </p>
                <blockquote class="callout" data-callout="question" data-callout-fold>
                  <div class="callout-title" dir="auto">
                    <div class="callout-title-inner" dir="auto">
                      <p dir="auto">
                        Question
                      </p>
                    </div>
                  </div>
                  <div class="callout-content" dir="auto">
                    <p dir="auto">
                      In the context of Gartner’s hype cycle, what has been the trajectory of generative conversational AI?
                    </p>
                    <p dir="auto">
                      Should a format including generative conversational AI be introduced to replace traditional essay assignments in educational settings,
                      and if so, what are some potential implications for student learning and assessment?
                      <cite class id="citation--dwivedi2023102642--1">
                        (
                        <a href="#bib-dwivedi2023102642-reader" data-no-popover="true" data-bib class="internal">
                          <span class="indicator-hook"></span>
                          Dwivedi et al., 2023
                        </a>
                        )
                      </cite>
                    </p>
                  </div>
                </blockquote>
                <h2 id="introduction-reader" dir="auto">
                  Introduction.
                  <a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#introduction-reader" class="internal">
                    <span class="indicator-hook"></span>
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                    </svg>
                  </a>
                </h2>
                <p dir="auto">
                  Historically, Alan Turing’s seminal work “Computing Machinery and Intelligence” laid the foundation for exploring the possibilities of a thinking machine
                  <cite class id="citation--10.1093/mind/lix.236.433--2">
                    (
                    <a href="#bib-10.1093/mind/lix.236.433-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      TURING, 1950
                    </a>
                    )
                  </cite>
                  .
                  Subsequently, the development of
                  <a href="../thoughts/Machine-learning" class="internal" data-slug="thoughts/Machine-learning" data-no-popover="true">
                    <span class="indicator-hook"></span>
                    AI
                  </a>
                  had taken a symbolic approach — world representations through systems that utilise high-level symbols and manipulate tokens to arrive at a result commonly referred to as Good Old-Fashioned AI (GOFAI)
                  <cite class id="citation--10.7551/mitpress/4626.001.0001--3">
                    (
                    <a href="#bib-10.7551/mitpress/4626.001.0001-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Haugeland, 1997
                    </a>
                    )
                  </cite>
                  .
                </p>
                <p dir="auto">
                  While GOFAI showed promise through decision-tree
                  <a href="../thoughts/reason" class="internal" data-slug="thoughts/reason" data-no-popover="true">
                    <span class="indicator-hook"></span>
                    reasoning
                  </a>
                  , its limitations became apparent in the 1980s when the field entered “AI Winter.” This was likely due to the cynicism within the AI researchers’ community and a reduction in funding, which halted most research and development
                  <cite class id="citation--handler2008avoidanotheraiwinter--4">
                    (
                    <a href="#bib-handler2008avoidanotheraiwinter-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Hendler, 2008
                    </a>
                    )
                  </cite>
                  .
                </p>
                <p dir="auto">
                  However, given the rise of Moore’s Law and the exponential amount of computing and
                  <a href="../thoughts/data" class="internal" data-slug="thoughts/data" data-no-popover="true">
                    <span class="indicator-hook"></span>
                    data
                  </a>
                  available, a new approach to
                  <a href="../thoughts/AGI" class="internal" data-slug="thoughts/AGI" data-no-popover="true">
                    <span class="indicator-hook"></span>
                    AI
                  </a>
                  arose, focusing on statistical methods and connectionist networks such as artificial neural networks.
                  <cite class id="citation--10.7551/mitpress/4626.001.0001--5">
                    (
                    <a href="#bib-10.7551/mitpress/4626.001.0001-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Haugeland, 1997
                    </a>
                    )
                  </cite>
                  dubbed this approach as New Fangled AI (NFAI). Fast forward to the
                  <span class="katex">
                    <span class="katex-mathml">
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <semantics>
                          <mrow>
                            <mn>
                              2
                            </mn>
                            <msup>
                              <mn>
                                1
                              </mn>
                              <mtext>
                                st
                              </mtext>
                            </msup>
                          </mrow>
                          <annotation encoding="application/x-tex">
                            21^{\text{st}}
                          </annotation>
                        </semantics>
                      </math>
                    </span>
                    <span class="katex-html" aria-hidden="true">
                      <span class="base">
                        <span class="strut" style="height:0.7936em;"></span>
                        <span class="mord">
                          2
                        </span>
                        <span class="mord">
                          <span class="mord">
                            1
                          </span>
                          <span class="msupsub">
                            <span class="vlist-t">
                              <span class="vlist-r">
                                <span class="vlist" style="height:0.7936em;">
                                  <span style="top:-3.063em;margin-right:0.05em;">
                                    <span class="pstrut" style="height:2.7em;"></span>
                                    <span class="sizing reset-size6 size3 mtight">
                                      <span class="mord mtight">
                                        <span class="mord text mtight">
                                          <span class="mord mtight">
                                            st
                                          </span>
                                        </span>
                                      </span>
                                    </span>
                                  </span>
                                </span>
                              </span>
                            </span>
                          </span>
                        </span>
                      </span>
                    </span>
                  </span>
                  century, ML has entered the mainstream through the rise of generative AI (GenAI).
                </p>
                <p dir="auto">
                  This paper posits that GenAI currently occupies the “peak of inflated expectations”, approaching the “trough of disillusionment” on Gartner’s hype cycle. It will also examine the implications of machine-assisted interfaces beyond conversational UI and their pedagogical consequences for student learning and assessment.
                </p>
                <h2 id="gartners-hype-cycle-reader" dir="auto">
                  Gartner’s hype cycle.
                  <a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#gartners-hype-cycle-reader" class="internal">
                    <span class="indicator-hook"></span>
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                    </svg>
                  </a>
                </h2>
                <p dir="auto">
                  For context, applications such as ChatGPT are built on top of
                  <a href="../thoughts/Transformers" class="internal" data-slug="thoughts/Transformers" data-no-popover="true">
                    <span class="indicator-hook"></span>
                    Transformers
                  </a>
                  architecture and pre-trained on a large corpus of
                  <a href="../thoughts/Language#representation" class="internal" data-slug="thoughts/Language" data-no-popover="true">
                    <span class="indicator-hook"></span>
                    text
                  </a>
                  <cite class id="citation--brown2020languagemodelsfewshotlearners--6">
                    (
                    <a href="#bib-brown2020languagemodelsfewshotlearners-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Brown et al., 2020
                    </a>
                    )
                  </cite>
                  . Given
                  an input sequence of tokens length
                  <span class="katex">
                    <span class="katex-mathml">
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <semantics>
                          <mrow>
                            <mi>
                              n
                            </mi>
                          </mrow>
                          <annotation encoding="application/x-tex">
                            n
                          </annotation>
                        </semantics>
                      </math>
                    </span>
                    <span class="katex-html" aria-hidden="true">
                      <span class="base">
                        <span class="strut" style="height:0.4306em;"></span>
                        <span class="mord mathnormal">
                          n
                        </span>
                      </span>
                    </span>
                  </span>
                  , these systems will predict the next tokens at index
                  <span class="katex">
                    <span class="katex-mathml">
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <semantics>
                          <mrow>
                            <mi>
                              n
                            </mi>
                            <mo>
                              +
                            </mo>
                            <mn>
                              1
                            </mn>
                          </mrow>
                          <annotation encoding="application/x-tex">
                            n+1
                          </annotation>
                        </semantics>
                      </math>
                    </span>
                    <span class="katex-html" aria-hidden="true">
                      <span class="base">
                        <span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span>
                        <span class="mord mathnormal">
                          n
                        </span>
                        <span class="mspace" style="margin-right:0.2222em;"></span>
                        <span class="mbin">
                          +
                        </span>
                        <span class="mspace" style="margin-right:0.2222em;"></span>
                      </span>
                      <span class="base">
                        <span class="strut" style="height:0.6444em;"></span>
                        <span class="mord">
                          1
                        </span>
                      </span>
                    </span>
                  </span>
                  . Most implementations of transformers are autoregressive
                  <cite class id="citation--croft2023llm--7">
                    (
                    <a href="#bib-croft2023llm-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Croft, 2023
                    </a>
                    )
                  </cite>
                  , meaning that the model will predict the future values (index
                  <span class="katex">
                    <span class="katex-mathml">
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <semantics>
                          <mrow>
                            <mi>
                              n
                            </mi>
                            <mo>
                              +
                            </mo>
                            <mn>
                              1
                            </mn>
                            <mo>
                              →
                            </mo>
                            <mi mathvariant="normal">
                              ∞
                            </mi>
                          </mrow>
                          <annotation encoding="application/x-tex">
                            n+1 \to \infty
                          </annotation>
                        </semantics>
                      </math>
                    </span>
                    <span class="katex-html" aria-hidden="true">
                      <span class="base">
                        <span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span>
                        <span class="mord mathnormal">
                          n
                        </span>
                        <span class="mspace" style="margin-right:0.2222em;"></span>
                        <span class="mbin">
                          +
                        </span>
                        <span class="mspace" style="margin-right:0.2222em;"></span>
                      </span>
                      <span class="base">
                        <span class="strut" style="height:0.6444em;"></span>
                        <span class="mord">
                          1
                        </span>
                        <span class="mspace" style="margin-right:0.2778em;"></span>
                        <span class="mrel">
                          →
                        </span>
                        <span class="mspace" style="margin-right:0.2778em;"></span>
                      </span>
                      <span class="base">
                        <span class="strut" style="height:0.4306em;"></span>
                        <span class="mord">
                          ∞
                        </span>
                      </span>
                    </span>
                  </span>
                  ) based on past values (index
                  <span class="katex">
                    <span class="katex-mathml">
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <semantics>
                          <mrow>
                            <mn>
                              0
                            </mn>
                            <mo>
                              →
                            </mo>
                            <mi>
                              n
                            </mi>
                          </mrow>
                          <annotation encoding="application/x-tex">
                            0 \to n
                          </annotation>
                        </semantics>
                      </math>
                    </span>
                    <span class="katex-html" aria-hidden="true">
                      <span class="base">
                        <span class="strut" style="height:0.6444em;"></span>
                        <span class="mord">
                          0
                        </span>
                        <span class="mspace" style="margin-right:0.2778em;"></span>
                        <span class="mrel">
                          →
                        </span>
                        <span class="mspace" style="margin-right:0.2778em;"></span>
                      </span>
                      <span class="base">
                        <span class="strut" style="height:0.4306em;"></span>
                        <span class="mord mathnormal">
                          n
                        </span>
                      </span>
                    </span>
                  </span>
                  ).
                  However,
                  <cite class id="citation--keles2022computationalcomplexityselfattention--8">
                    (
                    <a href="#bib-keles2022computationalcomplexityselfattention-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Keles et al., 2022, p. 4
                    </a>
                    )
                  </cite>
                  proved that the computation complexity of self-attention is quadratic; therefore, running these systems in production remains a scaling problem
                  <cite class id="citation--kaplan2020scalinglawsneurallanguage--9">
                    (
                    <a href="#bib-kaplan2020scalinglawsneurallanguage-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Kaplan et al., 2020
                    </a>
                    )
                  </cite>
                  .
                </p>
                <p dir="auto">
                  The current positioning of GenAI at the peak of inflated expectations aligns with the
                  <cite class id="citation--gartner2024multimodal--10">
                    (
                    <a href="#bib-gartner2024multimodal-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Gartner, 2024
                    </a>
                    )
                  </cite>
                  prediction. Three key factors support this assessment: rapid advancement in research, widespread enterprise adoption, and increased public awareness.
                  Ongoing research in GenAI, specifically language models, spans several topics, including mechanistic interpretability
                  <cite class id="citation--nanda2023concrete--11">
                    (
                    <a href="#bib-nanda2023concrete-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Nanda, 2023
                    </a>
                    )
                  </cite>
                  , which explores the inner workings of auto-regressive models, information retrieval techniques aimed to improve correctness and reduce hallucinations among LLM systems
                  <cite class id="citation--béchard2024reducinghallucinationstructuredoutputs--dhuliawala2023chainofverificationreduceshallucinationlarge--12">
                    (
                    <a href="#bib-béchard2024reducinghallucinationstructuredoutputs-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Béchard &amp; Ayala, 2024
                    </a>
                    ;
                    <a href="#bib-dhuliawala2023chainofverificationreduceshallucinationlarge-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Dhuliawala et al., 2023
                    </a>
                    )
                  </cite>
                  ,
                  as well as vested interests in multimodal applications of transformers
                  <cite class id="citation--xu2023multimodallearningtransformerssurvey--13">
                    (
                    <a href="#bib-xu2023multimodallearningtransformerssurvey-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Xu et al., 2023
                    </a>
                    )
                  </cite>
                  . Leading research labs, from Anthropic on their interpretability and alignment work
                  <cite class id="citation--elhage2022superposition--bricken2023monosemanticity--templeton2024scaling--14">
                    (
                    <a href="#bib-bricken2023monosemanticity-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Bricken et al., 2023
                    </a>
                    ;
                    <a href="#bib-elhage2022superposition-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Elhage et al., 2022
                    </a>
                    ;
                    <a href="#bib-templeton2024scaling-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Templeton et al., 2024
                    </a>
                    )
                  </cite>
                  , AI21’s Jamba with its innovative hybrid transformers architecture
                  <cite class id="citation--jambateam2024jamba15hybridtransformermambamodels--15">
                    (
                    <a href="#bib-jambateam2024jamba15hybridtransformermambamodels-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Team et al., 2024
                    </a>
                    )
                  </cite>
                  to open-weights models from
                  <a href="https://www.llama.com/" class="external alias">
                    <span class="indicator-hook"></span>
                    Meta
                  </a>
                  ,
                  <a href="https://deepmind.google/technologies/gemini/pro/" class="external alias">
                    <span class="indicator-hook"></span>
                    Google
                  </a>
                  continue lead redefine the boundaries of what these systems are capable of.
                </p>
                <p dir="auto">
                  Enterprise adoption is evident with Salesforce
                  <cite class id="citation--nijkamp2023xgen7btechnicalreport--16">
                    (
                    <a href="#bib-nijkamp2023xgen7btechnicalreport-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Nijkamp et al., 2023
                    </a>
                    )
                  </cite>
                  , Oracle’s
                  <a href="https://cohere.com/customer-stories/oracle" class="external alias">
                    <span class="indicator-hook"></span>
                    collaboration with Cohere
                  </a>
                  , and Microsoft’s Copilot for its 365 Product Suite. However, widespread implementation doesn’t necessarily equate to immediate, measurable productivity gains. Integrating these systems effectively into enterprise workflows to deliver tangible business value takes time and effort.
                </p>
                <p dir="auto">
                  Despite the field’s excitement, the current hype and expectations often exceed its reliable capabilities, especially for complex use cases. Significant challenges persist, including
                  hallucinations and lack of factual grounding
                  <cite class id="citation--huang2023surveyhallucinationlargelanguage--17">
                    (
                    <a href="#bib-huang2023surveyhallucinationlargelanguage-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Huang et al., 2023, p. 3
                    </a>
                    )
                  </cite>
                  . We observe such behaviours in ChatGPT, where the given knowledge cutoff prevents the systems from providing up-to-date information, which will “hallucinate” and provide inaccurate answers.
                  <cite class id="citation--dwivedi2023102642--18">
                    (
                    <a href="#bib-dwivedi2023102642-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Dwivedi et al., 2023, p. 4.4.9.1.2
                    </a>
                    )
                  </cite>
                </p>
                <p dir="auto">
                  As the field progresses towards the “trough of disillusionment” on Gartner’s hype cycle, a more realistic assessment of GenAI’s capabilities will likely emerge, paving the way for more effective applications.
                </p>
                <h2 id="implications-of-machine-assisted-interfaces-and-its-pedagogical-consequences-for-student-learning-and-assessment-reader" dir="auto">
                  Implications of machine-assisted interfaces and its pedagogical consequences for student learning and assessment.
                  <a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#implications-of-machine-assisted-interfaces-and-its-pedagogical-consequences-for-student-learning-and-assessment-reader" class="internal">
                    <span class="indicator-hook"></span>
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                    </svg>
                  </a>
                </h2>
                <p dir="auto">
                  The proliferation of conversational user interfaces (CUI) is based upon a simple heuristic of how
                  <a href="../thoughts/Autoregressive-models" class="internal" data-slug="thoughts/Autoregressive-models" data-no-popover="true">
                    <span class="indicator-hook"></span>
                    auto-regressive models
                  </a>
                  models surface their internal state through generating the next tokens.
                </p>
                <p dir="auto">
                  CUIs often prove frustrating when dealing with tasks requiring larger information sets. Additionally, for tasks that require frequent information retrieval (research, academic writing), CUIs are suboptimal as they compel users to maintain information in their working memory unnecessarily.
                  Pozdniakov proposed a framework that incorporate both application and interaction design, emphasizing manual alignment inputs from end users
                  <cite class id="citation--pozdniakov2024largelanguagemodelsmeet--19">
                    (
                    <a href="#bib-pozdniakov2024largelanguagemodelsmeet-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Pozdniakov et al., 2024, p. 3
                    </a>
                    )
                  </cite>
                  .
                  This approach, when applied replace traditional essay assignments, has two major implications for student learning and assessment: a shift in core competencies and collaborative assessment methods.
                </p>
                <p dir="auto">
                  With machine-assisted interfaces, students will need to develop stronger critical thinking skills to evaluate AI-generated content and formulate precise instructions.
                  The focus will shift towards the process of reaching desired outcomes and improving information retrieval skills. This shift aligns with the potential for machine-assisted proofs to solve novel problems, as discussed by
                  <cite class id="citation--tao2024machineassisted--20">
                    (
                    <a href="#bib-tao2024machineassisted-reader" data-no-popover="true" data-bib class="internal">
                      <span class="indicator-hook"></span>
                      Tao, 2024
                    </a>
                    )
                  </cite>
                  .
                </p>
                <p dir="auto">
                  These new interfaces will require instructors to adapt their evaluation methods. Assessment will need to consider students’ pace flexibility and their level of engagement with a given topic.
                  This approach encourages a more holistic, cross-disciplinary understanding, better preparing students for continuous learning in our rapidly evolving technological landscape.
                </p>
                <section data-references class="bibliography">
                  <h2 id="reference-label-reader" dir="auto">
                    References
                    <a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#reference-label-reader" class="internal">
                      <span class="indicator-hook"></span>
                      <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                        <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                      </svg>
                    </a>
                  </h2>
                  <ul dir="auto">
                    <li class="csl-entry" id="bib-béchard2024reducinghallucinationstructuredoutputs-reader">
                      Béchard, P., &amp; Ayala, O. M. (2024).
                      <i>
                        Reducing hallucination in structured outputs via Retrieval-Augmented Generation
                      </i>
                      . arXiv preprint arXiv:2404.08189
                      <a href="https://arxiv.org/abs/2404.08189" target="_blank" rel="noopener noreferrer" class="internal" data-arxiv-id="2404.08189" data-no-popover="true">
                        <span class="indicator-hook"></span>
                        arxiv
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-bricken2023monosemanticity-reader">
                      Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N., Anil, C., Denison, C., Askell, A., Lasenby, R., Wu, Y., Kravec, S., Schiefer, N., Maxwell, T., Joseph, N., Hatfield-Dodds, Z., Tamkin, A., Nguyen, K., … Olah, C. (2023). Towards Monosemanticity: Decomposing Language Models With Dictionary Learning.
                      <i>
                        Transformer Circuits Thread
                      </i>
                      .
                      <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html" target="_blank" rel="noopener noreferrer" class="csl-external-link external alias">
                        <span class="indicator-hook"></span>
                        [link]
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-brown2020languagemodelsfewshotlearners-reader">
                      Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020).
                      <i>
                        Language Models are Few-Shot Learners
                      </i>
                      . arXiv preprint arXiv:2005.14165
                      <a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener noreferrer" class="internal" data-arxiv-id="2005.14165" data-no-popover="true">
                        <span class="indicator-hook"></span>
                        arxiv
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-croft2023llm-reader">
                      Croft, B. (2023).
                      <i>
                        LLM Visualization
                      </i>
                      .
                      <a href="https://bbycroft.net/llm" target="_blank" rel="noopener noreferrer" class="csl-external-link external">
                        <span class="indicator-hook"></span>
                        https://bbycroft.net/llm
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-dhuliawala2023chainofverificationreduceshallucinationlarge-reader">
                      Dhuliawala, S., Komeili, M., Xu, J., Raileanu, R., Li, X., Celikyilmaz, A., &amp; Weston, J. (2023).
                      <i>
                        Chain-of-Verification Reduces Hallucination in Large Language Models
                      </i>
                      . arXiv preprint arXiv:2309.11495
                      <a href="https://arxiv.org/abs/2309.11495" target="_blank" rel="noopener noreferrer" class="internal" data-arxiv-id="2309.11495" data-no-popover="true">
                        <span class="indicator-hook"></span>
                        arxiv
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-dwivedi2023102642-reader">
                      Dwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, A. K., Baabdullah, A. M., Koohang, A., Raghavan, V., Ahuja, M., Albanna, H., Albashrawi, M. A., Al-Busaidi, A. S., Balakrishnan, J., Barlette, Y., Basu, S., Bose, I., Brooks, L., Buhalis, D., … Wright, R. (2023). Opinion Paper: “So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy.
                      <i>
                        International Journal of Information Management
                      </i>
                      ,
                      <i>
                        71
                      </i>
                      , 102642.
                      <a href="https://doi.org/https://doi.org/10.1016/j.ijinfomgt.2023.102642" target="_blank" rel="noopener noreferrer" class="csl-external-link external">
                        <span class="indicator-hook"></span>
                        https://doi.org/https://doi.org/10.1016/j.ijinfomgt.2023.102642
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-elhage2022superposition-reader">
                      Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S., Hatfield-Dodds, Z., Lasenby, R., Drain, D., Chen, C., Grosse, R., McCandlish, S., Kaplan, J., Amodei, D., Wattenberg, M., &amp; Olah, C. (2022). Toy Models of Superposition.
                      <i>
                        Transformer Circuits Thread
                      </i>
                      .
                      <a href="https://transformer-circuits.pub/2022/toy_model/index.html" target="_blank" rel="noopener noreferrer" class="csl-external-link external alias">
                        <span class="indicator-hook"></span>
                        [link]
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-gartner2024multimodal-reader">
                      Gartner. (2024).
                      <i>
                        Gartner Predicts 40 Percent of Generative AI Solutions Will Be Multimodal By 2027
                      </i>
                      .
                      <a href="https://www.gartner.com/en/newsroom/press-releases/2024-09-09-gartner-predicts-40-percent-of-generative-ai-solutions-will-be-multimodal-by-2027" target="_blank" rel="noopener noreferrer" class="csl-external-link external">
                        <span class="indicator-hook"></span>
                        https://www.gartner.com/en/newsroom/press-releases/2024-09-09-gartner-predicts-40-percent-of-generative-ai-solutions-will-be-multimodal-by-2027
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-10.7551/mitpress/4626.001.0001-reader">
                      Haugeland, J. (1997).
                      <i>
                        Mind Design II: Philosophy, Psychology, and Artificial Intelligence
                      </i>
                      . The MIT Press.
                      <a href="https://doi.org/10.7551/mitpress/4626.001.0001" target="_blank" rel="noopener noreferrer" class="csl-external-link external">
                        <span class="indicator-hook"></span>
                        https://doi.org/10.7551/mitpress/4626.001.0001
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-handler2008avoidanotheraiwinter-reader">
                      Hendler, J. (2008). Avoiding Another AI Winter.
                      <i>
                        IEEE Intelligent Systems
                      </i>
                      ,
                      <i>
                        23
                      </i>
                      (2), 2–4.
                      <a href="https://doi.org/10.1109/MIS.2008.20" target="_blank" rel="noopener noreferrer" class="csl-external-link external">
                        <span class="indicator-hook"></span>
                        https://doi.org/10.1109/MIS.2008.20
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-huang2023surveyhallucinationlargelanguage-reader">
                      Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., &amp; Liu, T. (2023).
                      <i>
                        A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions
                      </i>
                      . arXiv preprint arXiv:2311.05232
                      <a href="https://arxiv.org/abs/2311.05232" target="_blank" rel="noopener noreferrer" class="internal" data-arxiv-id="2311.05232" data-no-popover="true">
                        <span class="indicator-hook"></span>
                        arxiv
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-kaplan2020scalinglawsneurallanguage-reader">
                      Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., &amp; Amodei, D. (2020).
                      <i>
                        Scaling Laws for Neural Language Models
                      </i>
                      . arXiv preprint arXiv:2001.08361
                      <a href="https://arxiv.org/abs/2001.08361" target="_blank" rel="noopener noreferrer" class="internal" data-arxiv-id="2001.08361" data-no-popover="true">
                        <span class="indicator-hook"></span>
                        arxiv
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-keles2022computationalcomplexityselfattention-reader">
                      Keles, F. D., Wijewardena, P. M., &amp; Hegde, C. (2022).
                      <i>
                        On The Computational Complexity of Self-Attention
                      </i>
                      . arXiv preprint arXiv:2209.04881
                      <a href="https://arxiv.org/abs/2209.04881" target="_blank" rel="noopener noreferrer" class="internal" data-arxiv-id="2209.04881" data-no-popover="true">
                        <span class="indicator-hook"></span>
                        arxiv
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-nanda2023concrete-reader">
                      Nanda, N. (2023).
                      <i>
                        Concrete Steps to Get Started in Transformer Mechanistic Interpretability
                      </i>
                      .
                      <a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started" target="_blank" rel="noopener noreferrer" class="csl-external-link external">
                        <span class="indicator-hook"></span>
                        https://www.neelnanda.io/mechanistic-interpretability/getting-started
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-nijkamp2023xgen7btechnicalreport-reader">
                      Nijkamp, E., Xie, T., Hayashi, H., Pang, B., Xia, C., Xing, C., Vig, J., Yavuz, S., Laban, P., Krause, B., Purushwalkam, S., Niu, T., Kryściński, W., Murakhovs’ka, L., Choubey, P. K., Fabbri, A., Liu, Y., Meng, R., Tu, L., … Xiong, C. (2023).
                      <i>
                        XGen-7B Technical Report
                      </i>
                      . arXiv preprint arXiv:2309.03450
                      <a href="https://arxiv.org/abs/2309.03450" target="_blank" rel="noopener noreferrer" class="internal" data-arxiv-id="2309.03450" data-no-popover="true">
                        <span class="indicator-hook"></span>
                        arxiv
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-pozdniakov2024largelanguagemodelsmeet-reader">
                      Pozdniakov, S., Brazil, J., Abdi, S., Bakharia, A., Sadiq, S., Gasevic, D., Denny, P., &amp; Khosravi, H. (2024).
                      <i>
                        Large Language Models Meet User Interfaces: The Case of Provisioning Feedback
                      </i>
                      . arXiv preprint arXiv:2404.11072
                      <a href="https://arxiv.org/abs/2404.11072" target="_blank" rel="noopener noreferrer" class="internal" data-arxiv-id="2404.11072" data-no-popover="true">
                        <span class="indicator-hook"></span>
                        arxiv
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-tao2024machineassisted-reader">
                      Tao, T. (2024).
                      <i>
                        Machine-Assisted Proofs
                      </i>
                      .
                      <a href="https://terrytao.wordpress.com/wp-content/uploads/2024/03/machine-assisted-proof-notices.pdf" target="_blank" rel="noopener noreferrer" class="csl-external-link external">
                        <span class="indicator-hook"></span>
                        https://terrytao.wordpress.com/wp-content/uploads/2024/03/machine-assisted-proof-notices.pdf
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-jambateam2024jamba15hybridtransformermambamodels-reader">
                      Team, J., Lenz, B., Arazi, A., Bergman, A., Manevich, A., Peleg, B., Aviram, B., Almagor, C., Fridman, C., Padnos, D., Gissin, D., Jannai, D., Muhlgay, D., Zimberg, D., Gerber, E. M., Dolev, E., Krakovsky, E., Safahi, E., Schwartz, E., … Shoham, Y. (2024).
                      <i>
                        Jamba-1.5: Hybrid Transformer-Mamba Models at Scale
                      </i>
                      . arXiv preprint arXiv:2408.12570
                      <a href="https://arxiv.org/abs/2408.12570" target="_blank" rel="noopener noreferrer" class="internal" data-arxiv-id="2408.12570" data-no-popover="true">
                        <span class="indicator-hook"></span>
                        arxiv
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-templeton2024scaling-reader">
                      Templeton, A., Conerly, T., Marcus, J., Lindsey, J., Bricken, T., Chen, B., Pearce, A., Citro, C., Ameisen, E., Jones, A., Cunningham, H., Turner, N. L., McDougall, C., MacDiarmid, M., Freeman, C. D., Sumers, T. R., Rees, E., Batson, J., Jermyn, A., … Henighan, T. (2024). Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet.
                      <i>
                        Transformer Circuits Thread
                      </i>
                      .
                      <a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html" target="_blank" rel="noopener noreferrer" class="csl-external-link external alias">
                        <span class="indicator-hook"></span>
                        [link]
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-10.1093/mind/lix.236.433-reader">
                      TURING, A. M. (1950). I.—COMPUTING MACHINERY AND INTELLIGENCE.
                      <i>
                        Mind
                      </i>
                      ,
                      <i>
                        LIX
                      </i>
                      (236), 433–460.
                      <a href="https://doi.org/10.1093/mind/LIX.236.433" target="_blank" rel="noopener noreferrer" class="csl-external-link external">
                        <span class="indicator-hook"></span>
                        https://doi.org/10.1093/mind/LIX.236.433
                      </a>
                    </li>
                    <li class="csl-entry" id="bib-xu2023multimodallearningtransformerssurvey-reader">
                      Xu, P., Zhu, X., &amp; Clifton, D. A. (2023).
                      <i>
                        Multimodal Learning with Transformers: A Survey
                      </i>
                      . arXiv preprint arXiv:2206.06488
                      <a href="https://arxiv.org/abs/2206.06488" target="_blank" rel="noopener noreferrer" class="internal" data-arxiv-id="2206.06488" data-no-popover="true">
                        <span class="indicator-hook"></span>
                        arxiv
                      </a>
                    </li>
                  </ul>
                </section>
              </div>
            </div>
          </div>
          <div class="image-popup-modal" id="image-popup-modal">
            <div class="image-popup-backdrop"></div>
            <div class="image-popup-content">
              <button class="image-popup-close" aria-label="Close popup">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <line x1="18" y1="6" x2="6" y2="18"></line>
                  <line x1="6" y1="6" x2="18" y2="18"></line>
                </svg>
              </button>
              <img class="image-popup-img" src alt/>
            </div>
          </div>
          <div id="mermaid-container">
            <div class="mermaid-backdrop"></div>
            <div id="mermaid-space">
              <div class="mermaid-header">
                <button class="close-button" aria-label="close button">
                  <svg aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <line x1="18" y1="6" x2="6" y2="18"></line>
                    <line x1="6" y1="6" x2="18" y2="18"></line>
                  </svg>
                </button>
              </div>
              <div class="mermaid-content"></div>
            </div>
          </div>
        </section>
      </section>
    </main>
  </body>
  <script type="application/javascript">
    function i(){let l=this.parentElement;l.classList.toggle("is-collapsed");let e=l.classList.contains("is-collapsed")?this.scrollHeight:l.scrollHeight;l.style.maxHeight=e+"px";let s=l,t=l.parentElement;for(;t;){if(!t.classList.contains("callout"))return;let a=t.classList.contains("is-collapsed")?t.scrollHeight:t.scrollHeight+s.scrollHeight;t.style.maxHeight=a+"px",s=t,t=t.parentElement}}function r(l){let o=l.getBoundingClientRect(),e=1056,s=o.height;return o.top%e+s>e}function c(){let l=document.getElementsByClassName("callout is-collapsible");for(let e of l){let s=e.firstElementChild;if(s){s.addEventListener("click",i),window.addCleanup(()=>s.removeEventListener("click",i));let n=e.classList.contains("is-collapsed")?s.scrollHeight:e.scrollHeight;e.style.maxHeight=n+"px"}}let o=document.getElementsByClassName("callout");for(let e of o)r(e)&&(e.style.pageBreakBefore="always",e.classList.add("force-page-break"))}document.addEventListener("nav",c);window.addEventListener("resize",c);
  </script>
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" type="application/javascript"></script>
  <script src="../postscript.js" type="module"></script>
</html>