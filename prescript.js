(function(){var i=class{seed;m=2147483648;a=1103515245;c=42069;constructor(t){this.seed=t}next(){return this.seed=(this.a*this.seed+this.c)%this.m,this.seed/this.m}nextInt(t,a){return Math.floor(this.next()*(a-t)+t)}choice(t){return t[this.nextInt(0,t.length)]}shuffle(t){let a=[...t];for(let e=a.length-1;e>0;e--){let s=this.nextInt(0,e+1);[a[e],a[s]]=[a[s],a[e]]}return a}},l=[{name:"Cross Entropy Loss",latex:"\\mathcal{L} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)",asciimath:"L = -sum_(i=1)^n y_i log(hat y_i)",description:"Measures dissimilarity between predicted probabilities and true labels"},{name:"Gradient Descent",latex:"w_{t+1} = w_t - \\alpha \\nabla_{w_t} \\mathcal{L}(w_t)",asciimath:"w_(t+1) = w_t - alpha nabla_(w_t) L(w_t)",description:"Updates parameters in direction that minimizes loss"},{name:"Backpropagation",latex:"\\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial \\mathcal{L}}{\\partial a} \\frac{\\partial a}{\\partial z} \\frac{\\partial z}{\\partial w}",asciimath:"((del L)/(del w)) = ((del L)/(del a))((del a)/(del z))((del z)/(del w))",description:"Chain rule application in neural network backpropagation"},{name:"Softmax",latex:"\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}",asciimath:"sigma(bb{z})_i = (e^(z_i))/(sum_(j=1)^K e^(z_j))",description:"Converts logits to probabilities that sum to 1"},{name:"ReLU",latex:"f(x) = \\max(0, x)",asciimath:"f(x) = max(0, x)",description:"Rectified Linear Unit activation function"},{name:"LSTM Cell",latex:"\\begin{aligned} f_t &= \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) \\\\ i_t &= \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) \\\\ \\tilde{C}_t &= \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) \\\\ C_t &= f_t \\times C_{t-1} + i_t \\times \\tilde{C}_t \\end{aligned}",asciimath:`
      f_t = sigma(W_f * [h_(t-1), x_t] + b_f)
      i_t = sigma(W_i * [h_(t-1), x_t] + b_i)
      tilde C_t = tanh(W_C * [h_(t-1), x_t] + b_C)
      C_t = f_t times C_(t-1) + i_t times tilde C_t`,description:"Long Short-Term Memory cell computations"},{name:"Batch Normalization",latex:"\\hat{x}^{(k)} = \\frac{x^{(k)} - E[x^{(k)}]}{\\sqrt{Var[x^{(k)}] + \\epsilon}}",asciimath:"hat x^((k)) = (x^((k)) - E[x^((k))])/(sqrt(Var[x^((k))] + epsilon))",description:"Normalizes layer inputs for faster training"},{name:"Self-Attention",latex:"\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V",asciimath:"text(Attention)(Q,K,V) = text(softmax)((QK^T)/(sqrt(d_k)))V",description:"Scaled dot-product attention mechanism"},{name:"Adam Optimizer",latex:"\\begin{aligned} m_t &= \\beta_1 m_{t-1} + (1-\\beta_1) g_t \\\\ v_t &= \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 \\\\ \\hat{m}_t &= \\frac{m_t}{1-\\beta_1^t} \\\\ \\hat{v}_t &= \\frac{v_t}{1-\\beta_2^t} \\\\ w_{t+1} &= w_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t \\end{aligned}",asciimath:`
      m_t = beta_1 m_(t-1) + (1-beta_1)g_t
      v_t = beta_2 v_(t-1) + (1-beta_2)g_t^2
      hat m_t = m_t/(1-beta_1^t)
      hat v_t = v_t/(1-beta_2^t)
      w_(t+1) = w_t - (eta)/(sqrt(hat v_t) + epsilon) hat m_t`,description:"Adam optimizer parameter updates"},{name:"Dropout",latex:"\\text{dropout}(x) = \\begin{cases} \\frac{x}{1-p} & \\text{with probability } 1-p \\\\ 0 & \\text{with probability } p \\end{cases}",asciimath:`text(dropout)(x) = cases(
      (x)/(1-p) with"probability"1-p,
      0 with"probability"p
    )`,description:"Dropout regularization during training"},{name:"JumpReLU",latex:"\\text{JReLU}(x) = \\begin{cases} x + \\alpha & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}",asciimath:"text(JReLU)(x) = cases(x + alpha if x > 0, 0 otherwise)",description:"Jump ReLU adds a positive jump of \u03B1 to standard ReLU activation"},{name:"Nesterov Momentum",latex:"\\begin{aligned} v_{t+1} &= \\mu v_t - \\epsilon \\nabla f(\\theta_t + \\mu v_t) \\\\ \\theta_{t+1} &= \\theta_t + v_{t+1} \\end{aligned}",asciimath:`v_(t+1) = mu v_t - epsilon nabla f(theta_t + mu v_t)
theta_(t+1) = theta_t + v_(t+1)`,description:"Nesterov momentum looks ahead by calculating gradients at the predicted next position"},{name:"Swish Activation",latex:"\\text{Swish}(x) = x \\cdot \\sigma(\\beta x) = \\frac{x}{1 + e^{-\\beta x}}",asciimath:"text(Swish)(x) = x * sigma(beta x) = (x)/(1 + e^(-beta x))",description:"Self-gated activation function that varies between linear and ReLU-like behavior based on \u03B2"}];function o(){let t=new Date,a=t.getFullYear()*1e4+(t.getMonth()+1)*100+t.getDate();return new i(a).choice(l)}function c(t,a=80){t||(t="");let e=String(t).replace(/\r\n/g,`
`).split(`
`).map(n=>n.replace(/^\s+/,""));e.length===0&&e.push("");let s=Math.min(a,Math.max(...e.map(n=>n.length))),_=Math.max(s,20),d="-".repeat(_+4),h=`-${d}+`,p=`+${d}+`,x=e.map(n=>{let u=" ".repeat(_-n.length+2);return`|  ${n}${u}|`});return[h,...x,p].join(`
`)}function m(t){let a=new Date().toLocaleDateString("en-US",{weekday:"long",year:"numeric",month:"long",day:"numeric"}),e={title:"font-size: 14px; font-weight: bold; color: #059669;",name:"font-size: 12px; font-weight: bold; color: #6366f1;",equation:"font-family: monospace; color: #2563eb;",description:"color: #4b5563; font-style: italic;",latex:"font-family: monospace; color: #2563eb;"};console.log(`
%c\u{1F9EE} Equation of the Day - ${a}

%c${t.name}

%c${c(t.asciimath)}

%c${t.description}

%cLatex: ${t.latex}
`,e.title,e.name,e.equation,e.description,e.latex)}var r=!1;document.addEventListener("nav",()=>{if(!r){let t=o();m(t),r=!0}})})(),function(){document.addEventListener("nav",()=>{let i=document.documentElement,l=document.body.dataset.language,o=getComputedStyle(i),c=o.getPropertyValue("--bodyFont"),m=o.getPropertyValue("--codeFont"),r=o.getPropertyValue("--headerFont");switch(l){case"vi":i.style.setProperty("--headerFont",`"Playfair Display", ${r}`),i.style.setProperty("--bodyFont",`"Playfair Display", ${c}`),i.style.setProperty("--codeFont",`"Playfair Display", ${m}`);break;default:i.style.removeProperty("--headerFont"),i.style.removeProperty("--bodyFont"),i.style.removeProperty("--codeFont");break}})}();
